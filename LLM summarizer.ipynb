{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flGljhuUHeMt"
   },
   "source": [
    "# Package installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 170754,
     "status": "ok",
     "timestamp": 1753451234858,
     "user": {
      "displayName": "DJ Skoozie",
      "userId": "09186462150119057596"
     },
     "user_tz": 420
    },
    "id": "b76XV1z-Hg3J",
    "outputId": "88d5b673-6ed3-4b88-a8ea-ab9801d76b9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.97.0)\n",
      "Collecting PyMuPDF\n",
      "  Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyMuPDF\n",
      "Successfully installed PyMuPDF-1.26.3\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
      "Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.11.0.post1\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss\u001b[0m\u001b[31m\n",
      "\u001b[0mCollecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.3.14.tar.gz (51.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.14.1)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (2.0.2)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.14-cp311-cp311-linux_x86_64.whl size=4237783 sha256=de1b54974f28f8728d23c06f4c2e0f2b8b4591677f0e6183155e8e6a9b233b17\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/b6/cf/7315ec7b0149210d2d4447d9c3338b36d10e56a1ecddcd35c0\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: diskcache, llama-cpp-python\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.14\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (0.33.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (2025.7.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (2025.7.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai PyMuPDF\n",
    "!pip install nltk\n",
    "!pip install faiss-cpu\n",
    "!pip install faiss\n",
    "!pip install llama-cpp-python\n",
    "!pip install huggingface-hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rR9e343Otl1S"
   },
   "source": [
    "## Downlaoding the LLM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 684596,
     "status": "ok",
     "timestamp": 1753451984596,
     "user": {
      "displayName": "DJ Skoozie",
      "userId": "09186462150119057596"
     },
     "user_tz": 420
    },
    "id": "TgKAWue_tneI",
    "outputId": "a668d1d5-e726-4779-fcb3-f61e8c4f155e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/commands/download.py:139: FutureWarning: Ignoring --local-dir-use-symlinks. Downloading to a local directory does not use symlinks anymore.\n",
      "  warnings.warn(\n",
      "Downloading 'Meta-Llama-3.1-8B-Instruct-Q8_0.gguf' to 'models/.cache/huggingface/download/GlD94183cubaCFfaIZtjuXl-nqA=.9da71c45c90a821809821244d4971e5e5dfad7eb091f0b8ff0546392393b6283.incomplete'\n",
      "Meta-Llama-3.1-8B-Instruct-Q8_0.gguf: 100% 8.54G/8.54G [08:48<00:00, 16.1MB/s]\n",
      "Download complete. Moving file to models/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\n",
      "models/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\n",
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/commands/download.py:139: FutureWarning: Ignoring --local-dir-use-symlinks. Downloading to a local directory does not use symlinks anymore.\n",
      "  warnings.warn(\n",
      "Downloading 'mistral-7b-instruct-v0.2.Q8_0.gguf' to 'models/.cache/huggingface/download/X9t4it3iY7mZLWewMMFtDNNvgwI=.f326f5f4f137f3ad30f8c9cc21d4d39e54476583e8306ee2931d5a022cb85b06.incomplete'\n",
      "mistral-7b-instruct-v0.2.Q8_0.gguf: 100% 7.70G/7.70G [01:20<00:00, 95.5MB/s]\n",
      "Download complete. Moving file to models/mistral-7b-instruct-v0.2.Q8_0.gguf\n",
      "models/mistral-7b-instruct-v0.2.Q8_0.gguf\n",
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/commands/download.py:139: FutureWarning: Ignoring --local-dir-use-symlinks. Downloading to a local directory does not use symlinks anymore.\n",
      "  warnings.warn(\n",
      "Downloading 'llama-2-7b-chat.Q8_0.gguf' to 'models/.cache/huggingface/download/wsx0wnd1WnTDsJn2h2pR33po-m4=.f47dade5e86466edb66c5afe6f8e9fb1fbb2c292827b90bd46b7a1817d864bf2.incomplete'\n",
      "llama-2-7b-chat.Q8_0.gguf: 100% 7.16G/7.16G [01:11<00:00, 101MB/s]\n",
      "Download complete. Moving file to models/llama-2-7b-chat.Q8_0.gguf\n",
      "models/llama-2-7b-chat.Q8_0.gguf\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli download bartowski/Meta-Llama-3.1-8B-Instruct-GGUF Meta-Llama-3.1-8B-Instruct-Q8_0.gguf --local-dir models --local-dir-use-symlinks False\n",
    "!huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.2-GGUF mistral-7b-instruct-v0.2.Q8_0.gguf --local-dir models --local-dir-use-symlinks False\n",
    "!huggingface-cli download TheBloke/Llama-2-7b-Chat-GGUF llama-2-7b-chat.Q8_0.gguf --local-dir models --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQJCSVcEFI_a"
   },
   "source": [
    "# Package loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36046,
     "status": "ok",
     "timestamp": 1753452127102,
     "user": {
      "displayName": "DJ Skoozie",
      "userId": "09186462150119057596"
     },
     "user_tz": 420
    },
    "id": "IMLn5bhGbN5O",
    "outputId": "75ed8c34-2e2b-4b08-e79b-27195f0ba770"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import fitz  # PyMuPDF\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from openai import OpenAI\n",
    "# pretrained embedding model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss, numpy as np\n",
    "import os\n",
    "# llama integrations\n",
    "from llama_cpp import Llama\n",
    "import json\n",
    "import pickle\n",
    "# term frequency per page\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# set the api key and creating the gpt client\n",
    "api_key = \"<put your chatgpt api here>\"\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1PqyZyqtRFUQ"
   },
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1163,
     "status": "ok",
     "timestamp": 1753453446352,
     "user": {
      "displayName": "DJ Skoozie",
      "userId": "09186462150119057596"
     },
     "user_tz": 420
    },
    "id": "u6ZwzIeNRHUC",
    "outputId": "5115b51f-70dd-46e1-d058-ce3b5026e62b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# set here the path to where you are storing your files\n",
    "\n",
    "# --------- To run on Colab --------------\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "root_path = '/content/drive/<put the path to your files here>'\n",
    "\n",
    "# --------- To run on locally --------------\n",
    "#root_path = '<Replace this with the folder where you have your files>'\n",
    "\n",
    "# add to this list, the name of the files that you want to consider\n",
    "files_list = [\"file_1.pdf\", \"file_2.pdf\"] # an example\n",
    "#files_list=[ \"<Fill this list with the name of the files separated by ,\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojtML8WrUGDA"
   },
   "source": [
    "## Load content function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5dohhYkFT--a"
   },
   "source": [
    "Loading the file contents and creating the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 3241,
     "status": "ok",
     "timestamp": 1753453453407,
     "user": {
      "displayName": "DJ Skoozie",
      "userId": "09186462150119057596"
     },
     "user_tz": 420
    },
    "id": "kv8QRDVPUfWs"
   },
   "outputs": [],
   "source": [
    "# this function given the file_path to the file\n",
    "# will return the text/content of the file\n",
    "def create_corpus(file_path):\n",
    "\n",
    "  # extract the file name from the path\n",
    "  file_name = file_path.split('/')[-1]\n",
    "\n",
    "  # keeping two lists, one for the content and the other one\n",
    "  # for the metadata regarding that content\n",
    "  pages, meta_data = [], []\n",
    "\n",
    "  # open the document\n",
    "  doc = fitz.open(file_path)\n",
    "\n",
    "  # go over each page in the document\n",
    "  for page_num, page in enumerate(doc):\n",
    "\n",
    "    # read the content of the whole page\n",
    "    page_text = page.get_text()\n",
    "\n",
    "    # add the content of this page to the list\n",
    "    pages.append(page_text)\n",
    "\n",
    "    # add the metadata for this page to the list\n",
    "    meta_data.append({\"document\": file_name , \"page_num\": page_num})\n",
    "\n",
    "  # return the list of pages contents and the metadata\n",
    "  # related to each page\n",
    "  return pages, meta_data\n",
    "\n",
    "# storing the documents and the metadata\n",
    "documents, metadata = [], []\n",
    "\n",
    "# loop over the list of files and add them to the lists\n",
    "for file_path in files_list:\n",
    "  document_pages, document_metadata = create_corpus(root_path + file_path)\n",
    "  documents += document_pages\n",
    "  metadata += document_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QP5JAPcjb_bM"
   },
   "source": [
    "# Creating tf-idf and embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIBWQoB1cDZh"
   },
   "source": [
    "At this point given the pages content we will create two presentations\n",
    "\n",
    "1. tf-idf which is the term frequency and existence per page\n",
    "2. creating embedding for each page using a pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 29306,
     "status": "ok",
     "timestamp": 1753453487046,
     "user": {
      "displayName": "DJ Skoozie",
      "userId": "09186462150119057596"
     },
     "user_tz": 420
    },
    "id": "RjyJrZStVwdT"
   },
   "outputs": [],
   "source": [
    "# set these objects to be none first\n",
    "tfidf_obj, documents_tfidf, embedding_model, page_embeddings = None, None, None, None\n",
    "\n",
    "# a convention for the file name, 5 initials_ of each file_name\n",
    "pre_load_prefix = f'{root_path}pre_loading/{\"_\".join(map(lambda x: x.replace(\" \", \"\")[:5], sorted(files_list)))}'\n",
    "\n",
    "# check if the we have the files for this combo\n",
    "if os.path.isfile(f'{pre_load_prefix}_tfidf_obj.pkl'):\n",
    "    # load the objects and the embeddings\n",
    "    tfidf_obj = pickle.load(open(f'{pre_load_prefix}_tfidf_obj.pkl', 'rb'))\n",
    "    documents_tfidf = pickle.load(open(f'{pre_load_prefix}_documents_tfidf.pkl', 'rb'))\n",
    "    embedding_model = pickle.load(open(f'{pre_load_prefix}_embedding_model.pkl', 'rb'))\n",
    "    page_embeddings = pickle.load(open(f'{pre_load_prefix}_page_embeddings.pkl', 'rb'))\n",
    "\n",
    "else:\n",
    "\n",
    "  # if the directory doesn't exist, create it first\n",
    "  if not os.path.isdir(f'{root_path}pre_loading'):\n",
    "    os.makedirs(f'{root_path}pre_loading')\n",
    "\n",
    "  # creating the tfidf_obj\n",
    "  tfidf_obj = TfidfVectorizer()\n",
    "  # create the tfidf vectors\n",
    "  documents_tfidf = tfidf_obj.fit_transform(documents)\n",
    "  # a pretrained model to create page embeddings based on the tokens\n",
    "  # appeared in the pages\n",
    "  embedding_model = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\", trust_remote_code=True)\n",
    "  # get the embeddings from the model given the pages content\n",
    "  page_embeddings  = embedding_model.encode(documents, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "  # save them for later use\n",
    "  pickle.dump(tfidf_obj, open(f'{pre_load_prefix}_tfidf_obj.pkl', 'wb'))\n",
    "  pickle.dump(documents_tfidf, open(f'{pre_load_prefix}_documents_tfidf.pkl', 'wb'))\n",
    "  pickle.dump(embedding_model, open(f'{pre_load_prefix}_embedding_model.pkl', 'wb'))\n",
    "  pickle.dump(page_embeddings, open(f'{pre_load_prefix}_page_embeddings.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GiV9JbENgH_l"
   },
   "source": [
    "## FAISS indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vyIAXBHOgO8G"
   },
   "source": [
    "creating the indexes based on the page embeddings for matching to the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 71,
     "status": "ok",
     "timestamp": 1753453527682,
     "user": {
      "displayName": "DJ Skoozie",
      "userId": "09186462150119057596"
     },
     "user_tz": 420
    },
    "id": "Mns83GhIgYEL"
   },
   "outputs": [],
   "source": [
    "# creating the indexes based on the page embeddings\n",
    "# for the comparison\n",
    "index = faiss.IndexFlatIP(page_embeddings.shape[1])\n",
    "index.add(page_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LB18RLVrgZBz"
   },
   "source": [
    "## Search function + relevant retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gD9GrGVDgbN4"
   },
   "source": [
    "This function given the query and the objects and information necessary will return the pages that were relevant to the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1753453529811,
     "user": {
      "displayName": "DJ Skoozie",
      "userId": "09186462150119057596"
     },
     "user_tz": 420
    },
    "id": "xJ_E4ccpqmmr"
   },
   "outputs": [],
   "source": [
    "# given the query and additional information, will return the pages tha tare relevant to the query\n",
    "def hybrid_search_top_k(query, pages_metadata, tfidf_mat, tfidf_obj, page_embeddings, embedding_model, faiss_index, alpha=0.5, k=10):\n",
    "\n",
    "    # create the tfidf for the query - lexical part\n",
    "    q_bow = tfidf_obj.transform([query])\n",
    "\n",
    "    # create the embeddings for the query - semantic part\n",
    "    q_emb = embedding_model.encode([query], normalize_embeddings=True).astype('float32')\n",
    "\n",
    "    # find the top k*5 most related documents lexically\n",
    "    bow_scores = (q_bow @ tfidf_mat.T).toarray().ravel()\n",
    "    bow_idx    = bow_scores.argsort()[::-1][:k * 5]\n",
    "\n",
    "    # find the top k*5 top most related documents semantically\n",
    "    _, dense_idx = faiss_index.search(q_emb, k * 5)\n",
    "    dense_idx = dense_idx[0]\n",
    "\n",
    "    # taking the candidates proposed by these 2 approaches\n",
    "    candidates = np.unique(np.concatenate([dense_idx, bow_idx]))\n",
    "\n",
    "    # compute the final score for the proposed candidates, a weightening approach\n",
    "    # to prefer semantic or lexical retrieval\n",
    "    dense_scores = (page_embeddings[candidates] @ q_emb.T).ravel()     # cosine/IP\n",
    "    scores = alpha * bow_scores[candidates] + (1 - alpha) * dense_scores\n",
    "\n",
    "    # find the top k most relevant pages to the query\n",
    "    order = scores.argsort()[::-1][:k]\n",
    "    top_ids = candidates[order] # get their page number\n",
    "    top_scores = scores[order] # get their scores\n",
    "\n",
    "    # return the socre and the pages that are relevant to the query\n",
    "    results = [pages_metadata[i] | {\"score\": float(s)} | {\"page_idx\" : int(i)} for i, s in zip(top_ids, top_scores)]\n",
    "    return results\n",
    "\n",
    "def hybrid_search_top_percentile(query, pages_metadata, tfidf_mat, tfidf_obj, page_embeddings, embedding_model, faiss_index, alpha=0.5, percentile=90.0):\n",
    "    # create the tfidf for the query - lexical part\n",
    "    q_bow = tfidf_obj.transform([query])\n",
    "    bow_scores = (q_bow @ tfidf_mat.T).toarray().ravel()\n",
    "\n",
    "    # create the embeddings for the query - semantic part\n",
    "    q_emb = embedding_model.encode([query], normalize_embeddings=True).astype('float32')\n",
    "    dense_scores = (page_embeddings @ q_emb.T).ravel()  # cosine/IP\n",
    "\n",
    "    # combine the scores\n",
    "    combined_scores = alpha * bow_scores + (1 - alpha) * dense_scores\n",
    "\n",
    "    # find the threholds such that top p precentile be filtered\n",
    "    thresh = np.percentile(combined_scores, 100 - percentile)\n",
    "\n",
    "    # select the pages with score higher than the threshold\n",
    "    selected_idx = np.where(combined_scores >= thresh)[0]\n",
    "\n",
    "    # select the indexes based on the scores\n",
    "    sorted_idx = selected_idx[np.argsort(combined_scores[selected_idx])[::-1]]\n",
    "\n",
    "    # return the results\n",
    "    results = [{**pages_metadata[i], \"combined_score\": float(combined_scores[i]), \"page_idx\": int(i)}for i in sorted_idx]\n",
    "    return results\n",
    "\n",
    "# this function given the list of pages content\n",
    "# and the most relevant contents details will return\n",
    "# the text of all of those pages\n",
    "def relevant_pages_content(pages_content, relevant_pages):\n",
    "\n",
    "  # put together the text of the most relevant queris\n",
    "  relevant_pages_text = \"\"\" \"\"\"\n",
    "\n",
    "  # go over each page and put together the content of the page\n",
    "  for relevant_page in relevant_pages:\n",
    "\n",
    "    # get the page number\n",
    "    page_idx = relevant_page['page_idx']\n",
    "\n",
    "    # get the text of that page\n",
    "    page_text = pages_content[page_idx]\n",
    "\n",
    "    # add the text to the previous content of the text\n",
    "    relevant_pages_text += page_text + '\\n'\n",
    "\n",
    "    return relevant_pages_text\n",
    "\n",
    "# this function given a text will remove the stopwords from the text in order to\n",
    "# clean the text and reduce the size of it\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    #filtered_text = [word for word in words if word.lower() not in stop_words and word.isalpha()]\n",
    "    filtered_text = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOuoPNb6v1Py"
   },
   "source": [
    "# ChatGpt functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1Tx7zJyv6es"
   },
   "source": [
    "These functions given the text or the file, will get the summary of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1753453539639,
     "user": {
      "displayName": "DJ Skoozie",
      "userId": "09186462150119057596"
     },
     "user_tz": 420
    },
    "id": "EEla7xdR_7OD"
   },
   "outputs": [],
   "source": [
    "# this functin will send the text to the chatgpt and get the summary of the document\n",
    "# in a structured manner\n",
    "def summarize_text(text, query):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a document summarization assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"Write me the summary of the following document and point out the key points to fully respond to this query \"{query}\".\n",
    "\n",
    "              Return the output in the following structured JSON format:\n",
    "                {{\"summary\": \"<brief summary>\",\n",
    "                  \"key_points\": [\"<point 1>\", \"<point 2>\", \"...\"],\n",
    "                  \"section_titles\": [\"<optional section titles if available>\"]}}\n",
    "\n",
    "              Here is the document:\n",
    "              {text}\n",
    "             \"\"\"}\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# this function will analyze the answer given by the llm and\n",
    "# assign score to them\n",
    "def score_response_with_chatgpt(user_query, context_text, llm_response):\n",
    "\n",
    "    system_prompt = \"You are an expert evaluator.\"\n",
    "    user_prompt = f\"\"\"\n",
    "    Question: {user_query}\n",
    "\n",
    "    Context: {context_text}\n",
    "\n",
    "    Response: {llm_response}\n",
    "\n",
    "    Please evaluate the response on the following criteria, each from 0 to 10:\n",
    "    1. Relevance — Does it answer the question?\n",
    "    2. Accuracy — Is it supported by the context?\n",
    "    3. Clarity — Is it well-structured and easy to understand?\n",
    "\n",
    "    Also provide an overall score (0–10) and brief comments.\n",
    "\n",
    "    Return your evaluation in JSON format like this:\n",
    "    {{\n",
    "      \"relevance\": <int>,\n",
    "      \"accuracy\": <int>,\n",
    "      \"clarity\": <int>,\n",
    "      \"overall_score\": <int>,\n",
    "      \"comments\": \"<text>\"\n",
    "    }}\n",
    "    Always output valid JSON and nothing else.\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "    return json.loads(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEObEQ7ppqYF"
   },
   "source": [
    "# LLM Local functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 90,
     "status": "ok",
     "timestamp": 1753453555218,
     "user": {
      "displayName": "DJ Skoozie",
      "userId": "09186462150119057596"
     },
     "user_tz": 420
    },
    "id": "-Yk7KOT-xBsh"
   },
   "outputs": [],
   "source": [
    "# this function given the passed text and llm\n",
    "# will use the model in order to summarize the text\n",
    "def summarize_text_local(text, llm, query):\n",
    "    prompt = (f\"\"\"Write me the summary of the following document and point out the key points to fully respond to this query \"{query}\".\n",
    "\n",
    "              Return the output in the following structured JSON format:\n",
    "                {{\"summary\": \"<brief summary>\\n\",\n",
    "                  \"key_points\": [\"<point 1>\", \"<point 2>\", \"...\\n\"],\n",
    "                  \"section_titles\": [\"<optional section titles if available>\\n\"]}}\n",
    "\n",
    "              Here is the document:\n",
    "              {text}\n",
    "             \"\"\")\n",
    "\n",
    "    resp = llm.create_chat_completion(messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a document summarization assistant.\"},\n",
    "         {\"role\": \"user\", \"content\": prompt}\n",
    "        ],max_tokens = None, temperature=0.2, response_format={\"type\":\"json_object\"})\n",
    "\n",
    "    return resp['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9vvXnYqt6C0"
   },
   "source": [
    "# LLMs testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVGL8AQpt-W2"
   },
   "source": [
    "In this section we will set the parameter and the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 662,
     "status": "ok",
     "timestamp": 1753453560560,
     "user": {
      "displayName": "DJ Skoozie",
      "userId": "09186462150119057596"
     },
     "user_tz": 420
    },
    "id": "phTXDABnuJX-"
   },
   "outputs": [],
   "source": [
    "# these are going to be used for the serach parts\n",
    "# alpha in (0, 1): balance between the word matching vs semantic\n",
    "# higher values gives more importance to word matching\n",
    "top_k, top_percentile, alpha = 100, 10, 0.6\n",
    "\n",
    "# set the query of the user\n",
    "user_query = \"When is nitrogen used in any test?\"\n",
    "\n",
    "'''\n",
    "# if you want to find the top k most relevant pages\n",
    "relevant_pages = hybrid_search_top_k(query= user_query, pages_metadata=metadata,\n",
    "                        tfidf_mat=documents_tfidf,tfidf_obj=tfidf_obj,\n",
    "                        page_embeddings=page_embeddings,faiss_index=index,\n",
    "                        embedding_model=embedding_model, k = top_k, alpha=0.6)\n",
    "'''\n",
    "# if you want to find the top p% highest scored pages\n",
    "relevant_pages = hybrid_search_top_percentile(query= user_query, pages_metadata=metadata,\n",
    "                        tfidf_mat=documents_tfidf,tfidf_obj=tfidf_obj,\n",
    "                        page_embeddings=page_embeddings,faiss_index=index,\n",
    "                        embedding_model=embedding_model, percentile=top_percentile, alpha=alpha)\n",
    "\n",
    "# find the most relevant pages\n",
    "relevant_text = relevant_pages_content(documents, relevant_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owU7O0cJGEi-"
   },
   "source": [
    "## ChatGPT response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11183,
     "status": "ok",
     "timestamp": 1753453575257,
     "user": {
      "displayName": "DJ Skoozie",
      "userId": "09186462150119057596"
     },
     "user_tz": 420
    },
    "id": "2FVj7CoCWO_U",
    "outputId": "e08908f4-ab4f-496c-c996-3b00c842a228"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from ChatGPT:\n",
      "{\n",
      "  \"summary\": \"The document outlines procedures for strength testing of high-pressure pipelines and facilities, emphasizing the use of water as the primary test medium. However, when water is impractical, nitrogen, air, or natural gas can be used within specified limits. Nitrogen is particularly used when testing at lower stress levels or small volumes, and its required amount can be determined using specific guidelines or software. The document details the importance of selecting appropriate test procedures considering safety, environment, and equipment, and highlights the need for detailed written procedures, especially when back pressure and hydrostatic calculations are involved.\",\n",
      "  \"key_points\": [\n",
      "    \"Water is the preferred test medium for pressure tests addressing manufacturing and construction defects.\",\n",
      "    \"Nitrogen can be used as an alternative test medium when water is impractical, especially at lower stress levels or small volumes.\",\n",
      "    \"The amount of nitrogen required for testing can be determined using GS 182.050 or the DDS computer application.\",\n",
      "    \"Mixtures of air and nitrogen are permitted, but air and natural gas mixtures are not allowed.\",\n",
      "    \"Proper test procedures must consider safety, environment, test pressure, volume, elevation, and equipment.\",\n",
      "    \"Detailed written procedures are required, especially when back pressure and hydrostatic calculations are involved.\"\n",
      "  ],\n",
      "  \"section_titles\": [\"Strength Testing Procedures\", \"Gas Test Mediums\"]\n",
      "}\n",
      "\n",
      "\n",
      "ChatGPT response score: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'relevance': 9,\n",
       " 'accuracy': 10,\n",
       " 'clarity': 9,\n",
       " 'overall_score': 9,\n",
       " 'comments': 'The response effectively addresses the question by explaining when nitrogen is used in testing, supported by specific details from the context. It clearly distinguishes the conditions under which nitrogen is employed and provides relevant procedural information. Minor improvements could include more explicit emphasis on the specific testing scenarios for nitrogen use, but overall, it is comprehensive and well-structured.'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the response from ChatGPT\n",
    "gpt_reponse = summarize_text(relevant_text, user_query)\n",
    "\n",
    "print('Response from ChatGPT:')\n",
    "print(gpt_reponse, end='\\n\\n\\n')\n",
    "print(\"ChatGPT response score: \")\n",
    "score_response_with_chatgpt(user_query, relevant_text, gpt_reponse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I723JnpCw-9F"
   },
   "source": [
    "## Llama response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xeD8iYqeqOU5",
    "outputId": "ff8d324d-5906-463d-9dfb-5648c9337c61"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# load the model\n",
    "llama_Q8 = Llama(model_path=\"models/llama-2-7b-chat.Q8_0.gguf\",  n_ctx=0, verbose=False)\n",
    "\n",
    "# get the summary of the relevant text\n",
    "llama_Q8_response = summarize_text_local(relevant_text, llama_Q8, user_query)\n",
    "\n",
    "print('Response from Llama:')\n",
    "print(llama_Q8_response, end='\\n\\n\\n')\n",
    "print(\"Llama response score: \")\n",
    "print(score_response_with_chatgpt(user_query, relevant_text, llama_Q8_response))\n",
    "del llama_Q8 # release the memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFcWuAkU0R8p"
   },
   "source": [
    "## Meta Llama response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 178515,
     "status": "ok",
     "timestamp": 1752927533537,
     "user": {
      "displayName": "Mehrdad HASSANZADEH",
      "userId": "02222335473307374907"
     },
     "user_tz": -120
    },
    "id": "isVGqPur4ezD",
    "outputId": "2492ffa7-244c-4e1f-9717-afdae7c87da7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from Meta Llama:\n",
      "{\"summary\": \"This document outlines the requirements for hydrostatic testing of high-pressure pipelines and facilities. Prior to starting a hydrotest, several documents and preparations are necessary to ensure safety and compliance with company standards.\",\n",
      "\"key_points\": [\"Verify water used for hydrostatic testing meets GS 167.0232 requirements\", \"Contact Pipeline Engineering for significant elevation changes\", \"Verify flanges are torqued properly per GS 180.0020\", \"Avoid using hoses for filling and dewatering when practicable\", \"Use rated hoses and fittings for pumps\", \"Anchor dewatering piping to prevent movement\", \"Verify employees and contractors are aware of hazards\", \"Limit access to hydrostatic test dewatering system\", \"Use sufficient drying pigs/swabs to remove free-standing water\", \"Dry test segment using an air compressor or other approved methods\"],\n",
      "\"section_titles\": [\"4.6.2 Water used for hydrostatic testing\", \"4.6.3 Contact Pipeline Engineering\", \"4.6.4 Verify flanges\", \"4.6.5 Avoid using hoses\", \"4.6.6 Use rated hoses and fittings\", \"4.6.7 Anchor dewatering piping\", \"4.6.8 Verify employee awareness\", \"4.6.9 Limit access to hydrostatic test dewatering system\", \"4.6.10 Use sufficient drying pigs/swabs\", \"4.6.11 Dry test segment using an air compressor\"]}\n",
      "\n",
      "\n",
      "Meta Llama response score: \n",
      "{'relevance': 8, 'accuracy': 9, 'clarity': 8, 'overall_score': 8, 'comments': 'The response provides a comprehensive list of documents and preparations needed prior to hydrotesting, such as verifying water quality, flange torque, and equipment inspection. It is well-supported by the context and clearly structured. However, it could be slightly more explicit about the specific documents required (e.g., testing procedures, inspection reports, permits) rather than focusing mainly on procedural steps. Overall, it effectively addresses the question with minor room for improvement in specificity.'}\n",
      "CPU times: user 13min 9s, sys: 10.6 s, total: 13min 20s\n",
      "Wall time: 2min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Meta_llama = Llama(model_path=\"models/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\", n_ctx=0, verbose=False)\n",
    "\n",
    "# get the summary of the relevant text\n",
    "Meta_llama_response = summarize_text_local(relevant_text, Meta_llama, user_query)\n",
    "\n",
    "print('Response from Meta Llama:')\n",
    "print(Meta_llama_response, end='\\n\\n\\n')\n",
    "print(\"Meta Llama response score: \")\n",
    "print(score_response_with_chatgpt(user_query, relevant_text, Meta_llama_response))\n",
    "del Meta_llama # release the memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqgWW3uZAyXU"
   },
   "source": [
    "## Mistral response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "YXICzbwLBEpA",
    "outputId": "0b59bf0b-aa0f-4be9-dce7-bd96b8b9745c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from Mistral LLM:\n",
      "{\"summary\": \"This document outlines the requirements for hydrostatic testing of high pressure pipelines and facilities according to Southern California Gas Company's operations standard. Key points include: abiding by the liquid sampling and analysis requirements (GS 167.0232), contacting Pipeline Engineering for significant elevation changes, verifying flange torque, avoiding hose use when possible, using rated hoses and fittings, securing dewatering piping, limiting access to the hydrostatic test dewatering system, using sufficient drying pigs or swabs, and drying the test segment using approved methods.\",\n",
      "\"key_points\": [\n",
      "\"Abide by the requirements in GS 167.0232 for field sampling and analysis of liquids and solids/sludge\",\n",
      "\"Contact Pipeline Engineering for significant elevation changes\",\n",
      "\"Verify all flanges have been torqued in proper sequence\",\n",
      "\"Avoid hose use when practicable\",\n",
      "\"Use rated hoses and fittings for filling and dewatering\",\n",
      "\"Securely anchor/secure dewatering piping to prevent movement and separation\",\n",
      "\"Limit access to hydrostatic test dewatering system to necessary personnel\",\n",
      "\"Use sufficient drying pigs or swabs to remove all free-standing water\",\n",
      "\"Dry the test segment using approved methods\"\n",
      "],\n",
      "\"section_titles\": [\"Company Operations Standard\", \"Gas Standard\", \"Gas Engineering\", \"Strength Testing - High Pressure Pipelines and Facilities\"]\n",
      "}\n",
      "\n",
      "\n",
      "Mistral LLM response score: \n",
      "{'relevance': 9, 'accuracy': 9, 'clarity': 8, 'overall_score': 8, 'comments': 'The response effectively summarizes the key documents and procedural requirements prior to hydrotesting, aligning well with the provided context. It clearly lists the main points and sections, making it easy to understand. However, it could improve slightly in clarity by explicitly stating that these are the documents and steps needed before starting the hydrotest, rather than focusing mainly on procedures during the test. Overall, it provides a comprehensive and relevant answer supported by the context.'}\n",
      "CPU times: user 14min 44s, sys: 5.8 s, total: 14min 50s\n",
      "Wall time: 3min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# load the llm model\n",
    "Mistral_llm = Llama(model_path=\"models/mistral-7b-instruct-v0.2.Q8_0.gguf\", n_ctx=0, verbose=False)\n",
    "\n",
    "# get the summary of the relevant text\n",
    "Mistral_llm_response = summarize_text_local(relevant_text, Mistral_llm, user_query)\n",
    "\n",
    "print('Response from Mistral LLM:')\n",
    "print(Mistral_llm_response, end='\\n\\n\\n')\n",
    "print(\"Mistral LLM response score: \")\n",
    "print(score_response_with_chatgpt(user_query, relevant_text, Mistral_llm_response))\n",
    "del Mistral_llm # release the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "QmXy3jVNmU5Z"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
