{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mehrdadhz-77/SearchEngine-Summarizer/blob/main/Search_engine_summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the necessary packages"
      ],
      "metadata": {
        "id": "EQJCSVcEFI_a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMLn5bhGbN5O",
        "outputId": "03498ed1-edf5-4c3a-9f08-4ed495781024"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.86.0)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (1.26.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "!pip install openai PyMuPDF # to read pdf\n",
        "!pip install nltk # for natural language processing to clean the text\n",
        "!pip install faiss-cpu\n",
        "!pip install faiss # to perform the similarity check using the facebook similarity search\n",
        "import nltk\n",
        "import fitz  # PyMuPDF\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from openai import OpenAI\n",
        "\n",
        "# pretrained embedding model\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss, numpy as np\n",
        "\n",
        "# term frequency per page\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# set the api key and creating the gpt client\n",
        "api_key = '<your_api>'\n",
        "client = OpenAI(api_key=api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the data"
      ],
      "metadata": {
        "id": "1PqyZyqtRFUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# setting the root path\n",
        "root_path = '/content/drive/MyDrive/Python_Students_SuperProf/Darren/files/'\n",
        "\n",
        "# file 1 path\n",
        "file1_path = '2024 Contractor Safety Manual v2024.1.pdf'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6ZwzIeNRHUC",
        "outputId": "cf465e4c-9509-48e5-bd9e-936b7d05e0a4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load content function"
      ],
      "metadata": {
        "id": "ojtML8WrUGDA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function given the file path, will load the file and create a dictionary containing the content and metadata of the pages within the document"
      ],
      "metadata": {
        "id": "5dohhYkFT--a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this function given the file_path to the file\n",
        "# will return the text/content of the file\n",
        "def create_corpus(file_path):\n",
        "\n",
        "  # extract the file name from the path\n",
        "  file_name = file_path.split('/')[-1]\n",
        "\n",
        "  # keeping two lists, one for the content and the other one\n",
        "  # for the metadata regarding that content\n",
        "  pages, meta_data = [], []\n",
        "\n",
        "  # open the document\n",
        "  doc = fitz.open(file_path)\n",
        "\n",
        "  # go over each page in the document\n",
        "  for page_num, page in enumerate(doc):\n",
        "\n",
        "    # read the content of the whole page\n",
        "    page_text = page.get_text()\n",
        "\n",
        "    # add the content of this page to the list\n",
        "    pages.append(page_text)\n",
        "\n",
        "    # add the metadata for this page to the list\n",
        "    meta_data.append({\"document\": file_name , \"page_num\": page_num})\n",
        "\n",
        "  # return the list of pages contents and the metadata\n",
        "  # related to each page\n",
        "  return pages, meta_data\n",
        "\n",
        "\n",
        "# get the pages and the meta data of the first file\n",
        "file1_pages, file1_metadata = create_corpus(root_path + file1_path)"
      ],
      "metadata": {
        "id": "kv8QRDVPUfWs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating tf-idf and embeddings"
      ],
      "metadata": {
        "id": "QP5JAPcjb_bM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point given the pages content we will create two presentations\n",
        "\n",
        "1. tf-idf which is the term frequency and existence per page\n",
        "2. creating embedding for each page using a pre-trained model"
      ],
      "metadata": {
        "id": "AIBWQoB1cDZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the tfidf_obj\n",
        "tfidf_obj = TfidfVectorizer()\n",
        "\n",
        "# create the tfidf vectors\n",
        "file1_tfidf = tfidf_obj.fit_transform(file1_pages)\n",
        "\n",
        "# a pretrained model to create page embeddings based on the tokens\n",
        "# appeared in the pages\n",
        "embedding_model = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\", trust_remote_code=True)\n",
        "\n",
        "# get the embeddings from the model given the pages content\n",
        "page_embeddings  = embedding_model.encode(file1_pages, convert_to_numpy=True, normalize_embeddings=True)"
      ],
      "metadata": {
        "id": "RjyJrZStVwdT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FAISS indexing"
      ],
      "metadata": {
        "id": "GiV9JbENgH_l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "creating the indexes based on the page embeddings for matching to the query"
      ],
      "metadata": {
        "id": "vyIAXBHOgO8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the indexes based on the page embeddings\n",
        "# for the comparison\n",
        "index = faiss.IndexFlatIP(page_embeddings.shape[1])\n",
        "index.add(page_embeddings)"
      ],
      "metadata": {
        "id": "Mns83GhIgYEL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Search function + relevant retrieval"
      ],
      "metadata": {
        "id": "LB18RLVrgZBz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function given the query and the objects and information necessary will return the pages that were relevant to the query"
      ],
      "metadata": {
        "id": "gD9GrGVDgbN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# given the query and additional information, will return the pages tha tare relevant to the query\n",
        "def hybrid_search(query, pages_metadata, tfidf_mat, tfidf_obj, page_embeddings, embedding_model, faiss_index, alpha=0.5, k=10):\n",
        "\n",
        "    # create the tfidf for the query - lexical part\n",
        "    q_bow = tfidf_obj.transform([query])\n",
        "\n",
        "    # create the embeddings for the query - semantic part\n",
        "    q_emb = embedding_model.encode([query], normalize_embeddings=True).astype('float32')\n",
        "\n",
        "    # find the top k*5 most related documents lexically\n",
        "    bow_scores = (q_bow @ tfidf_mat.T).toarray().ravel()\n",
        "    bow_idx    = bow_scores.argsort()[::-1][:k * 5]\n",
        "\n",
        "    # find the top k*5 top most related documents semantically\n",
        "    _, dense_idx = faiss_index.search(q_emb, k * 5)\n",
        "    dense_idx = dense_idx[0]\n",
        "\n",
        "    # taking the candidates proposed by these 2 approaches\n",
        "    candidates = np.unique(np.concatenate([dense_idx, bow_idx]))\n",
        "\n",
        "    # compute the final score for the proposed candidates, a weightening approach\n",
        "    # to prefer semantic or lexical retrieval\n",
        "    dense_scores = (page_embeddings[candidates] @ q_emb.T).ravel()     # cosine/IP\n",
        "    scores = alpha * bow_scores[candidates] + (1 - alpha) * dense_scores\n",
        "\n",
        "    # find the top k most relevant pages to the query\n",
        "    order = scores.argsort()[::-1][:k]\n",
        "    top_ids = candidates[order] # get their page number\n",
        "    top_scores = scores[order] # get their scores\n",
        "\n",
        "    # return the socre and the pages that are relevant to the query\n",
        "    results = [pages_metadata[i] | {\"score\": float(s)} for i, s in zip(top_ids, top_scores)]\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "xJ_E4ccpqmmr"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passing the necessary information, retrieving the most relevant pages to the query"
      ],
      "metadata": {
        "id": "nT6iOXllvdKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the query of the user\n",
        "user_query = \"prohibited materials\"\n",
        "\n",
        "# get the most relevant queries\n",
        "relevant_pages = hybrid_search(query= user_query, pages_metadata=file1_metadata,\n",
        "                        tfidf_mat=file1_tfidf,tfidf_obj=tfidf_obj,\n",
        "                        page_embeddings=page_embeddings,faiss_index=index,\n",
        "                        embedding_model=embedding_model)\n",
        "relevant_pages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FVj7CoCWO_U",
        "outputId": "afa91dd1-e959-4afd-dac9-a5dbf8c03c56"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'document': '2024 Contractor Safety Manual v2024.1.pdf',\n",
              "  'page_num': 45,\n",
              "  'score': 0.47815319456145944},\n",
              " {'document': '2024 Contractor Safety Manual v2024.1.pdf',\n",
              "  'page_num': 51,\n",
              "  'score': 0.3584377305109834},\n",
              " {'document': '2024 Contractor Safety Manual v2024.1.pdf',\n",
              "  'page_num': 2,\n",
              "  'score': 0.3550077685604948},\n",
              " {'document': '2024 Contractor Safety Manual v2024.1.pdf',\n",
              "  'page_num': 39,\n",
              "  'score': 0.3529007066313828},\n",
              " {'document': '2024 Contractor Safety Manual v2024.1.pdf',\n",
              "  'page_num': 28,\n",
              "  'score': 0.3473269376948872},\n",
              " {'document': '2024 Contractor Safety Manual v2024.1.pdf',\n",
              "  'page_num': 27,\n",
              "  'score': 0.33917852127782},\n",
              " {'document': '2024 Contractor Safety Manual v2024.1.pdf',\n",
              "  'page_num': 17,\n",
              "  'score': 0.33883731327023703},\n",
              " {'document': '2024 Contractor Safety Manual v2024.1.pdf',\n",
              "  'page_num': 22,\n",
              "  'score': 0.33492576352679143},\n",
              " {'document': '2024 Contractor Safety Manual v2024.1.pdf',\n",
              "  'page_num': 35,\n",
              "  'score': 0.33136773066964464},\n",
              " {'document': '2024 Contractor Safety Manual v2024.1.pdf',\n",
              "  'page_num': 38,\n",
              "  'score': 0.3267972457112735}]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChatGPT api"
      ],
      "metadata": {
        "id": "wOuoPNb6v1Py"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These functions given the text or the file, will get the summary of the text"
      ],
      "metadata": {
        "id": "N1Tx7zJyv6es"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this functin will send the text to the chatgpt and get the summary of the document\n",
        "# in a structured manner\n",
        "def summarize_text(text):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4.1-nano\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a document summarization assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"\"\"Write me the summary of the following document and point out the key points.\n",
        "\n",
        "              Return the output in the following structured JSON format:\n",
        "                {{\"summary\": \"<brief summary>\",\n",
        "                  \"key_points\": [\"<point 1>\", \"<point 2>\", \"...\"],\n",
        "                  \"section_titles\": [\"<optional section titles if available>\"]}}\n",
        "\n",
        "              Here is the document:\n",
        "              {text}\n",
        "             \"\"\"}\n",
        "        ],\n",
        "        temperature=0.5,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "# this function given the two text of documents will compare\n",
        "# the content of the two documents\n",
        "def compare_documents(text1, text2, file_name_1, file_name_2):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4.1-nano\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a document comparison assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"\"\"Compare the following two documents and summarize their similarities and differences. Use the full name of the file name when you want to return to each of the documents in ''. Return the result in structured JSON format with the following fields:\n",
        "                                         {{ \"similarities\": [\"<point 1>\", \"<point 2>\", \"...\"],\n",
        "                                            \"differences\": [\"<point 1>\", \"<point 2>\", \"...\"],\n",
        "                                            \"overlapping_themes\": [\"<theme 1>\", \"...\"]}}\n",
        "\n",
        "                                            --- filename: {file_name_1} ---\n",
        "                                            content: {text1}\n",
        "\n",
        "                                            --- filename: {file_name_2} ---\n",
        "                                            content: {text2}\"\"\"}\n",
        "        ],\n",
        "        temperature=0.5,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "# this function given a text will remove the stopwords from the text in order to\n",
        "# clean the text and reduce the size of it\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "    #filtered_text = [word for word in words if word.lower() not in stop_words and word.isalpha()]\n",
        "    filtered_text = [word for word in words if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_text)\n"
      ],
      "metadata": {
        "id": "EEla7xdR_7OD"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary of the relevant pages"
      ],
      "metadata": {
        "id": "owU7O0cJGEi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this function given the list of pages content\n",
        "# and the most relevant contents details will return\n",
        "# the text of all of those pages\n",
        "def relevant_pages_content(pages_content, relevant_pages):\n",
        "\n",
        "  # put together the text of the most relevant queris\n",
        "  relevant_pages_text = \"\"\" \"\"\"\n",
        "\n",
        "  # go over each page and put together the content of the page\n",
        "  for relevant_page in relevant_pages:\n",
        "\n",
        "    # get the page number\n",
        "    page_num = relevant_page['page_num']\n",
        "\n",
        "    # get the text of that page\n",
        "    page_text = pages_content[page_num]\n",
        "\n",
        "    # add the text to the previous content of the text\n",
        "    relevant_pages_text += page_text + '\\n'\n",
        "\n",
        "    return remove_stopwords(relevant_pages_text)\n",
        "\n",
        "\n",
        "# get the text of all of the relevant pages\n",
        "relevant_text = relevant_pages_content(file1_pages, relevant_pages)\n",
        "\n",
        "\n",
        "# get the summary of the relevant text\n",
        "print(summarize_text(relevant_text))"
      ],
      "metadata": {
        "id": "rT7RKs3hwIjj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c414ff3a-b1b5-47e0-b342-f0de0f2833b5"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"summary\": \"The Contractor Safety Manual outlines safety and conduct policies for contractors working with SoCalGas. It prohibits animals, dangerous reptiles, and the use of certain equipment in facilities. The manual bans illegal drugs, firearms, explosives, and alcohol on company property and reserves the right to inspect for prohibited materials. Photography and recording devices are allowed with prior approval.\",\n",
            "  \"key_points\": [\n",
            "    \"Prohibition of animals, dangerous reptiles, and certain equipment in facilities.\",\n",
            "    \"Strict ban on illegal drugs, firearms, explosives, and alcohol on SoCalGas property.\",\n",
            "    \"SoCalGas reserves the right to inspect brought-in materials for prohibited items.\",\n",
            "    \"Photography and audio/video recording devices require prior approval.\"\n",
            "  ],\n",
            "  \"section_titles\": [\"Prohibited Materials\", \"Photography/Camera Devices\"]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M6ZOOIuOx4ns"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}